# Project Map

## Complete File Structure

```
7030CEM/
â”‚
â”œâ”€â”€ ðŸ“‹ Documentation (6 files)
â”‚   â”œâ”€â”€ README.md                          Main documentation (comprehensive guide)
â”‚   â”œâ”€â”€ QUICKSTART.md                      Quick start guide (getting started)
â”‚   â”œâ”€â”€ PROJECT_OVERVIEW.md                Detailed architecture overview
â”‚   â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md          Implementation details
â”‚   â”œâ”€â”€ PROJECT_MAP.md                     This file (navigation guide)
â”‚   â””â”€â”€ LICENSE                            MIT License
â”‚
â”œâ”€â”€ âš™ï¸ Configuration (2 files)
â”‚   â”œâ”€â”€ config.yaml                        Main configuration file
â”‚   â””â”€â”€ requirements.txt                   Python dependencies
â”‚
â”œâ”€â”€ ðŸš€ Main Executables (2 files)
â”‚   â”œâ”€â”€ run_all.py                         Run complete pipeline
â”‚   â””â”€â”€ example_usage.py                   Usage examples
â”‚
â”œâ”€â”€ ðŸ Source Code (src/)
â”‚   â”œâ”€â”€ __init__.py                        Package initialization
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“¦ parsers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ drain.py                       Drain log parser implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“¦ preprocessing/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ parser.py                      HDFS-specific log parser
â”‚   â”‚   â”œâ”€â”€ template_mapper.py             Event template mapping
â”‚   â”‚   â”œâ”€â”€ sequence_builder.py            Block-wise sequence builder
â”‚   â”‚   â”œâ”€â”€ data_splitter.py               Train/test data splitter
â”‚   â”‚   â””â”€â”€ text_converter.py              Sequence to text converter
â”‚   â”‚
â”‚   â”œâ”€â”€ ðŸ“¦ model/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ inference.py                   Model loading and inference
â”‚   â”‚
â”‚   â””â”€â”€ ðŸ“¦ evaluation/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ metrics.py                     Comprehensive metrics computation
â”‚
â”œâ”€â”€ ðŸ“œ Scripts (scripts/)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ download_data.py                   Download HDFS dataset
â”‚   â”œâ”€â”€ preprocess.py                      Run preprocessing pipeline
â”‚   â”œâ”€â”€ benchmark.py                       Run model benchmarking
â”‚   â”œâ”€â”€ view_results.py                    Display results
â”‚   â””â”€â”€ check_setup.py                     Verify project setup
â”‚
â”œâ”€â”€ ðŸ“ Data Directory (datasets/)
â”‚   â”œâ”€â”€ .gitkeep                           Git placeholder
â”‚   â””â”€â”€ hdfs/                              (Created by download_data.py)
â”‚       â”œâ”€â”€ HDFS.log                       Raw HDFS logs (~16GB)
â”‚       â”œâ”€â”€ anomaly_label.csv              Ground truth labels
â”‚       â””â”€â”€ output/hdfs/                   Processed files
â”‚           â”œâ”€â”€ HDFS.log_structured.csv    Parsed logs
â”‚           â”œâ”€â”€ HDFS.log_templates.csv     Event templates
â”‚           â”œâ”€â”€ hdfs_log_templates.json    Template mapping
â”‚           â”œâ”€â”€ hdfs_sequence.csv          Block sequences
â”‚           â”œâ”€â”€ hdfs_sequence_labeled.csv  With labels
â”‚           â”œâ”€â”€ hdfs_text.csv              Text format
â”‚           â”œâ”€â”€ hdfs_train.csv             Training set
â”‚           â””â”€â”€ hdfs_test.csv              Test set
â”‚
â””â”€â”€ ðŸ“Š Results Directory (results/)
    â”œâ”€â”€ .gitkeep                           Git placeholder
    â”œâ”€â”€ benchmark_results.json             (Created by benchmark.py)
    â””â”€â”€ predictions.csv                    Per-sample predictions

```

## File Purposes

### ðŸ“‹ Documentation Files

| File | Purpose | When to Read |
|------|---------|--------------|
| `README.md` | Complete project documentation | First time setup |
| `QUICKSTART.md` | Step-by-step getting started | Want to start quickly |
| `PROJECT_OVERVIEW.md` | Architecture and design details | Understanding system design |
| `IMPLEMENTATION_SUMMARY.md` | Implementation details and stats | Understanding what was built |
| `PROJECT_MAP.md` | File structure navigation | Finding specific files |
| `LICENSE` | Legal license information | Understanding usage rights |

### âš™ï¸ Configuration Files

| File | Purpose | When to Edit |
|------|---------|--------------|
| `config.yaml` | Main configuration | Changing model/preprocessing settings |
| `requirements.txt` | Python dependencies | Adding new packages |

### ðŸš€ Main Executables

| File | Purpose | When to Run |
|------|---------|-------------|
| `run_all.py` | Complete pipeline | First time or full benchmark |
| `example_usage.py` | Usage examples | Learning how to use components |

### ðŸ Source Code Modules

#### parsers/
- `drain.py` - Implements Drain algorithm for log parsing
  - **Key Classes**: `Drain`, `LogCluster`, `Node`
  - **Main Function**: `parse()` - Parse logs and extract templates

#### preprocessing/
- `parser.py` - HDFS log parsing
  - **Key Class**: `HDFSLogParser`
  - **Main Function**: `parse_file()` - Parse entire log file
  
- `template_mapper.py` - Template ID mapping
  - **Key Class**: `TemplateMapper`
  - **Main Function**: `create_mapping()` - Create numerical mapping
  
- `sequence_builder.py` - Build log sequences
  - **Key Class**: `SequenceBuilder`
  - **Main Function**: `build_sequences()` - Create block sequences
  
- `data_splitter.py` - Split train/test
  - **Key Class**: `DataSplitter`
  - **Main Function**: `split_data()` - Split into train/test sets
  
- `text_converter.py` - Convert to text
  - **Key Class**: `TextConverter`
  - **Main Function**: `convert_sequence()` - Number to text

#### model/
- `inference.py` - Model inference
  - **Key Class**: `LogAnomalyDetector`
  - **Main Function**: `predict()` - Run inference

#### evaluation/
- `metrics.py` - Metrics computation
  - **Key Class**: `BenchmarkMetrics`
  - **Main Function**: `compute_all_metrics()` - Calculate all metrics

### ðŸ“œ Scripts

| Script | Purpose | Input | Output | Time |
|--------|---------|-------|--------|------|
| `download_data.py` | Download dataset | URL | HDFS logs | 5-10 min |
| `preprocess.py` | Preprocess data | Raw logs | Processed files | 10-30 min |
| `benchmark.py` | Run benchmark | Test set | Results JSON | 5-15 min |
| `view_results.py` | Display results | Results JSON | Formatted output | Instant |
| `check_setup.py` | Verify setup | Project files | Status report | Instant |

## Workflow Navigation

### For First Time Users

1. **Start Here**: `QUICKSTART.md`
2. **Then Read**: `README.md` (skim sections)
3. **Run**: `python scripts/check_setup.py`
4. **Execute**: `python run_all.py`
5. **View**: `python scripts/view_results.py`

### For Developers

1. **Architecture**: `PROJECT_OVERVIEW.md`
2. **Implementation**: `IMPLEMENTATION_SUMMARY.md`
3. **Code**: Browse `src/` directory
4. **Examples**: `example_usage.py`
5. **Extend**: Modify relevant modules

### For Researchers

1. **Methodology**: `README.md` â†’ "Preprocessing Pipeline" section
2. **Run Benchmark**: `python run_all.py`
3. **Analyze**: `results/benchmark_results.json`
4. **Compare**: Check confusion matrix and metrics
5. **Investigate**: `results/predictions.csv` for error analysis

## Code Navigation Guide

### Want to understand...

**Log Parsing?**
- Read: `src/parsers/drain.py`
- See: `example_usage.py` â†’ `example_parse_logs()`

**Data Processing?**
- Read: `src/preprocessing/` directory
- Start with: `parser.py` â†’ `sequence_builder.py` â†’ `text_converter.py`

**Model Inference?**
- Read: `src/model/inference.py`
- See: `example_usage.py` â†’ `example_model_inference()`

**Evaluation Metrics?**
- Read: `src/evaluation/metrics.py`
- See: `example_usage.py` â†’ `example_metrics_computation()`

**Complete Pipeline?**
- Read: `scripts/preprocess.py` â†’ `scripts/benchmark.py`
- Or: `run_all.py`

## Quick Reference

### File Size Guide

| File Type | Typical Size | Location |
|-----------|--------------|----------|
| Python modules | 5-15 KB | `src/` |
| Scripts | 3-8 KB | `scripts/` |
| Documentation | 5-50 KB | Root directory |
| Raw HDFS logs | 16 GB | `datasets/hdfs/` |
| Processed data | 100-500 MB | `datasets/hdfs/output/` |
| Results | 10-150 MB | `results/` |

### Line Count Guide

| Module | Approx Lines | Complexity |
|--------|--------------|------------|
| `drain.py` | 300+ | High |
| `parser.py` | 200+ | Medium |
| `inference.py` | 150+ | Medium |
| `metrics.py` | 400+ | High |
| Other modules | 100-200 | Low-Medium |

## Import Paths

### For Scripts
```python
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from preprocessing import HDFSLogParser
from model import LogAnomalyDetector
from evaluation import BenchmarkMetrics
```

### For External Use
```python
# Add project to path
import sys
sys.path.insert(0, '/path/to/7030CEM/src')

# Import modules
from parsers import Drain
from preprocessing import (
    HDFSLogParser,
    TemplateMapper,
    SequenceBuilder,
    DataSplitter,
    TextConverter
)
from model import LogAnomalyDetector
from evaluation import BenchmarkMetrics
```

## Dependency Tree

```
run_all.py
  â””â”€> scripts/download_data.py
  â””â”€> scripts/preprocess.py
        â””â”€> src/parsers/drain.py
        â””â”€> src/preprocessing/parser.py
        â””â”€> src/preprocessing/template_mapper.py
        â””â”€> src/preprocessing/sequence_builder.py
        â””â”€> src/preprocessing/data_splitter.py
        â””â”€> src/preprocessing/text_converter.py
  â””â”€> scripts/benchmark.py
        â””â”€> src/model/inference.py
        â””â”€> src/evaluation/metrics.py
```

## Configuration Dependencies

```
config.yaml
  â”œâ”€> model (used by: scripts/benchmark.py)
  â”œâ”€> dataset (used by: scripts/download_data.py)
  â”œâ”€> output (used by: all scripts)
  â”œâ”€> preprocessing (used by: scripts/preprocess.py)
  â””â”€> benchmark (used by: scripts/benchmark.py)
```

## Data Flow Diagram

```
[Raw Logs]
    â†“
[download_data.py]
    â†“
[HDFS.log + anomaly_label.csv]
    â†“
[preprocess.py]
    â”œâ”€> [drain.py] â†’ Templates
    â”œâ”€> [parser.py] â†’ Structured logs
    â”œâ”€> [template_mapper.py] â†’ Mapping
    â”œâ”€> [sequence_builder.py] â†’ Sequences
    â””â”€> [data_splitter.py] â†’ Train/Test
    â†“
[hdfs_test.csv]
    â†“
[benchmark.py]
    â”œâ”€> [inference.py] â†’ Predictions
    â””â”€> [metrics.py] â†’ Results
    â†“
[benchmark_results.json]
    â†“
[view_results.py]
    â†“
[Formatted Output]
```

## Git Workflow

### Tracked Files
- All `.py` files
- All `.md` files
- `config.yaml`
- `requirements.txt`
- `LICENSE`
- `.gitkeep` files

### Ignored Files (.gitignore)
- `datasets/` (except `.gitkeep`)
- `results/` (except `.gitkeep`)
- `__pycache__/`
- `*.pyc`
- `.venv/`
- IDE files

## Search Index

**Find configuration**: â†’ `config.yaml`  
**Find dependencies**: â†’ `requirements.txt`  
**Find documentation**: â†’ Root `*.md` files  
**Find log parser**: â†’ `src/parsers/drain.py`  
**Find preprocessing**: â†’ `src/preprocessing/`  
**Find model code**: â†’ `src/model/inference.py`  
**Find metrics**: â†’ `src/evaluation/metrics.py`  
**Find scripts**: â†’ `scripts/`  
**Find examples**: â†’ `example_usage.py`  
**Find data**: â†’ `datasets/hdfs/`  
**Find results**: â†’ `results/`  

---

**Last Updated**: October 2024  
**Total Files**: 32  
**Total Directories**: 8  
**Project Status**: âœ… Complete

