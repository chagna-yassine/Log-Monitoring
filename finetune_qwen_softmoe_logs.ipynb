{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning Qwen2.5-72B-Instruct with SoftMoE & Quantization for Log Understanding\n",
    "\n",
    "This notebook fine-tunes **Qwen/Qwen2.5-72B-Instruct** using:\n",
    "- ‚ö° **4-bit Quantization** (QLoRA) for efficient training on large models\n",
    "- üß† **Soft Mixture of Experts (SoftMoE)** for specialized log understanding\n",
    "- üìä **Multi-task Learning** for:\n",
    "  - Log Parsing (extracting templates)\n",
    "  - Log Summarization (summarizing sequences)\n",
    "  - Log Classification (anomaly detection)\n",
    "\n",
    "## Requirements\n",
    "- GPU: T4 (free), V100, or A100 recommended\n",
    "- RAM: 12GB+ system RAM\n",
    "- Storage: ~30GB for model + data\n",
    "\n",
    "## Author\n",
    "Created for 7030CEM - Log Understanding Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running on Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"‚úÖ Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"‚ö†Ô∏è  Not running on Colab\")\n",
    "\n",
    "# Mount Google Drive (optional - for saving checkpoints)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"‚úÖ Google Drive mounted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "!pip install -q -U transformers accelerate peft bitsandbytes\n",
    "!pip install -q -U datasets huggingface_hub\n",
    "!pip install -q -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install -q -U trl sentencepiece protobuf\n",
    "!pip install -q -U wandb tensorboard\n",
    "!pip install -q -U einops scipy\n",
    "\n",
    "print(\"‚úÖ All packages installed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType,\n",
    ")\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Check CUDA availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "@dataclass\n",
    "class Config:\n",
    "    # Model settings\n",
    "    model_name: str = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "    \n",
    "    # Dataset settings - CHANGE THIS TO YOUR HUGGINGFACE DATASET\n",
    "    dataset_name: str = \"chYassine/ait-fox-raw-logs\"  # Your HF dataset\n",
    "    \n",
    "    # Quantization settings\n",
    "    use_4bit: bool = True\n",
    "    bnb_4bit_compute_dtype: str = \"bfloat16\"\n",
    "    bnb_4bit_quant_type: str = \"nf4\"  # Normal Float 4\n",
    "    use_nested_quant: bool = True  # Double quantization\n",
    "    \n",
    "    # LoRA settings\n",
    "    lora_r: int = 64  # LoRA rank\n",
    "    lora_alpha: int = 128  # LoRA alpha (scaling)\n",
    "    lora_dropout: float = 0.1\n",
    "    \n",
    "    # SoftMoE settings\n",
    "    num_experts: int = 8  # Number of experts\n",
    "    num_experts_per_token: int = 2  # Active experts per token\n",
    "    use_softmoe: bool = True\n",
    "    \n",
    "    # Training settings\n",
    "    max_seq_length: int = 2048\n",
    "    num_train_epochs: int = 3\n",
    "    per_device_train_batch_size: int = 1\n",
    "    per_device_eval_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 16\n",
    "    learning_rate: float = 2e-5\n",
    "    warmup_steps: int = 100\n",
    "    weight_decay: float = 0.01\n",
    "    \n",
    "    # Output settings\n",
    "    output_dir: str = \"./qwen-log-understanding\"\n",
    "    logging_steps: int = 10\n",
    "    save_steps: int = 500\n",
    "    eval_steps: int = 500\n",
    "    \n",
    "    # HuggingFace token\n",
    "    hf_token: Optional[str] = None  # Will be set from secrets\n",
    "\n",
    "config = Config()\n",
    "print(\"‚úÖ Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup HuggingFace authentication\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login to HuggingFace\n",
    "if IN_COLAB:\n",
    "    from google.colab import userdata\n",
    "    try:\n",
    "        hf_token = userdata.get('HF_TOKEN')\n",
    "        login(token=hf_token)\n",
    "        print(\"‚úÖ Logged in to HuggingFace using Colab secrets\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  No HF_TOKEN found in Colab secrets\")\n",
    "        print(\"Please run: from huggingface_hub import login; login()\")\n",
    "        login()\n",
    "else:\n",
    "    print(\"Please authenticate with HuggingFace:\")\n",
    "    login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Mixture of Experts (SoftMoE) Implementation\n",
    "\n",
    "SoftMoE uses soft routing instead of discrete gating, allowing smoother gradient flow and better expert utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SoftMoELayer(nn.Module):\n",
    "    \"\"\"Soft Mixture of Experts Layer with smooth routing.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        num_experts: int = 8,\n",
    "        expert_capacity: int = 2,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.expert_capacity = expert_capacity\n",
    "        \n",
    "        # Router network\n",
    "        self.router = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_experts),\n",
    "        )\n",
    "        \n",
    "        # Expert networks\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(hidden_size, hidden_size * 4),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(hidden_size * 4, hidden_size),\n",
    "                nn.Dropout(dropout),\n",
    "            )\n",
    "            for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "        # Layer norm\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hidden_states: (batch_size, seq_len, hidden_size)\n",
    "        Returns:\n",
    "            output: (batch_size, seq_len, hidden_size)\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, hidden_size = hidden_states.shape\n",
    "        \n",
    "        # Compute routing weights (soft routing)\n",
    "        router_logits = self.router(hidden_states)  # (B, S, num_experts)\n",
    "        router_weights = F.softmax(router_logits, dim=-1)  # Soft routing\n",
    "        \n",
    "        # Select top-k experts per token\n",
    "        topk_weights, topk_indices = torch.topk(\n",
    "            router_weights, self.expert_capacity, dim=-1\n",
    "        )  # (B, S, expert_capacity)\n",
    "        \n",
    "        # Normalize top-k weights\n",
    "        topk_weights = F.softmax(topk_weights, dim=-1)\n",
    "        \n",
    "        # Process through experts\n",
    "        expert_outputs = []\n",
    "        for i in range(self.expert_capacity):\n",
    "            expert_idx = topk_indices[:, :, i]  # (B, S)\n",
    "            expert_weight = topk_weights[:, :, i:i+1]  # (B, S, 1)\n",
    "            \n",
    "            # Gather expert outputs\n",
    "            expert_output = torch.zeros_like(hidden_states)\n",
    "            for expert_id in range(self.num_experts):\n",
    "                mask = (expert_idx == expert_id).unsqueeze(-1)  # (B, S, 1)\n",
    "                expert_result = self.experts[expert_id](hidden_states)\n",
    "                expert_output = expert_output + mask.float() * expert_result\n",
    "            \n",
    "            expert_outputs.append(expert_weight * expert_output)\n",
    "        \n",
    "        # Combine expert outputs\n",
    "        output = sum(expert_outputs)\n",
    "        \n",
    "        # Residual connection and layer norm\n",
    "        output = self.layer_norm(hidden_states + output)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"‚úÖ SoftMoE implementation loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from HuggingFace\n",
    "print(f\"Loading dataset: {config.dataset_name}\")\n",
    "\n",
    "try:\n",
    "    raw_dataset = load_dataset(config.dataset_name)\n",
    "    print(f\"‚úÖ Dataset loaded successfully\")\n",
    "    print(f\"Dataset info: {raw_dataset}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    print(\"\\nCreating sample dataset for demonstration...\")\n",
    "    \n",
    "    # Create a sample dataset if the real one isn't available\n",
    "    sample_data = {\n",
    "        'content': [\n",
    "            '2024-01-01 10:00:00 INFO Starting service on port 8080',\n",
    "            '2024-01-01 10:00:01 ERROR Connection refused to database',\n",
    "            '2024-01-01 10:00:02 WARN High memory usage detected: 85%',\n",
    "        ] * 100,  # Repeat for more examples\n",
    "        'host': ['server1', 'server2', 'server3'] * 100,\n",
    "        'log_type': ['system', 'database', 'system'] * 100,\n",
    "    }\n",
    "    raw_dataset = DatasetDict({\n",
    "        'train': Dataset.from_dict(sample_data),\n",
    "    })\n",
    "    print(\"‚úÖ Sample dataset created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample from dataset\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE DATA\")\n",
    "print(\"=\"*80)\n",
    "train_data = raw_dataset['train'] if 'train' in raw_dataset else raw_dataset\n",
    "for i in range(min(3, len(train_data))):\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    for key, value in train_data[i].items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Multi-Task Training Data\n",
    "\n",
    "We'll create three types of tasks:\n",
    "1. **Log Parsing**: Extract structured templates from raw logs\n",
    "2. **Log Summarization**: Summarize sequences of log events\n",
    "3. **Log Classification**: Classify logs as normal/anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class LogTaskGenerator:\n",
    "    \"\"\"Generate multi-task training examples from log data.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.parsing_patterns = [\n",
    "            (r'\\d{4}-\\d{2}-\\d{2}', '<DATE>'),\n",
    "            (r'\\d{2}:\\d{2}:\\d{2}', '<TIME>'),\n",
    "            (r'\\d+\\.\\d+\\.\\d+\\.\\d+', '<IP>'),\n",
    "            (r'\\b\\d+\\b', '<NUM>'),\n",
    "            (r'/[\\w/]+', '<PATH>'),\n",
    "        ]\n",
    "    \n",
    "    def extract_template(self, log_line: str) -> str:\n",
    "        \"\"\"Extract log template by replacing variables.\"\"\"\n",
    "        template = log_line\n",
    "        for pattern, placeholder in self.parsing_patterns:\n",
    "            template = re.sub(pattern, placeholder, template)\n",
    "        return template\n",
    "    \n",
    "    def create_parsing_task(self, log_line: str) -> Dict[str, str]:\n",
    "        \"\"\"Create log parsing task.\"\"\"\n",
    "        template = self.extract_template(log_line)\n",
    "        \n",
    "        instruction = (\n",
    "            \"You are a log parsing expert. Extract the template from the following log line by \"\n",
    "            \"replacing variable parts (dates, times, IPs, numbers, paths) with placeholders.\\n\\n\"\n",
    "            f\"Log line: {log_line}\\n\\n\"\n",
    "            \"Provide only the extracted template.\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'task': 'parsing',\n",
    "            'instruction': instruction,\n",
    "            'response': template,\n",
    "        }\n",
    "    \n",
    "    def create_classification_task(self, log_line: str, log_type: str = None) -> Dict[str, str]:\n",
    "        \"\"\"Create log classification task.\"\"\"\n",
    "        # Simple heuristic for anomaly detection\n",
    "        anomaly_keywords = ['error', 'fail', 'exception', 'critical', 'fatal', 'refused', 'timeout']\n",
    "        is_anomaly = any(keyword in log_line.lower() for keyword in anomaly_keywords)\n",
    "        label = 'anomaly' if is_anomaly else 'normal'\n",
    "        \n",
    "        instruction = (\n",
    "            \"You are a log analysis expert. Classify the following log entry as 'normal' or 'anomaly'.\\n\\n\"\n",
    "            f\"Log entry: {log_line}\\n\\n\"\n",
    "            \"Classification:\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'task': 'classification',\n",
    "            'instruction': instruction,\n",
    "            'response': label,\n",
    "        }\n",
    "    \n",
    "    def create_summarization_task(self, log_lines: List[str]) -> Dict[str, str]:\n",
    "        \"\"\"Create log summarization task.\"\"\"\n",
    "        # Create a simple summary\n",
    "        templates = [self.extract_template(line) for line in log_lines]\n",
    "        unique_templates = list(dict.fromkeys(templates))  # Preserve order\n",
    "        \n",
    "        summary = f\"This sequence contains {len(log_lines)} log entries with {len(unique_templates)} unique event types.\\n\"\n",
    "        summary += \"Main events: \" + \", \".join(unique_templates[:3])\n",
    "        \n",
    "        instruction = (\n",
    "            \"You are a log analysis expert. Summarize the following sequence of log entries.\\n\\n\"\n",
    "            f\"Logs:\\n\" + \"\\n\".join(f\"{i+1}. {line}\" for i, line in enumerate(log_lines[:10])) + \"\\n\\n\"\n",
    "            \"Summary:\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'task': 'summarization',\n",
    "            'instruction': instruction,\n",
    "            'response': summary,\n",
    "        }\n",
    "\n",
    "task_generator = LogTaskGenerator()\n",
    "print(\"‚úÖ Task generator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multi-task training examples\n",
    "def generate_training_examples(dataset, num_examples: int = 2000):\n",
    "    \"\"\"Generate multi-task training examples from dataset.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    # Ensure we have the right split\n",
    "    data = dataset['train'] if 'train' in dataset else dataset\n",
    "    \n",
    "    # Sample logs\n",
    "    num_examples = min(num_examples, len(data))\n",
    "    indices = np.random.choice(len(data), num_examples, replace=False)\n",
    "    \n",
    "    for idx in indices:\n",
    "        sample = data[int(idx)]\n",
    "        log_line = sample.get('content', '')\n",
    "        \n",
    "        if not log_line or (isinstance(log_line, str) and log_line.startswith('[BINARY')):\n",
    "            continue\n",
    "        \n",
    "        # Create parsing task (40% of examples)\n",
    "        if np.random.random() < 0.4:\n",
    "            examples.append(task_generator.create_parsing_task(log_line))\n",
    "        \n",
    "        # Create classification task (40% of examples)\n",
    "        elif np.random.random() < 0.75:\n",
    "            log_type = sample.get('log_type', None)\n",
    "            examples.append(task_generator.create_classification_task(log_line, log_type))\n",
    "        \n",
    "        # Create summarization task (20% of examples)\n",
    "        else:\n",
    "            # Get a sequence of logs\n",
    "            start = max(0, int(idx) - 5)\n",
    "            end = min(len(data), int(idx) + 5)\n",
    "            log_sequence = [data[i].get('content', '') for i in range(start, end)]\n",
    "            log_sequence = [l for l in log_sequence if l and not (isinstance(l, str) and l.startswith('[BINARY'))]\n",
    "            \n",
    "            if len(log_sequence) >= 3:\n",
    "                examples.append(task_generator.create_summarization_task(log_sequence))\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Generate examples\n",
    "print(\"Generating training examples...\")\n",
    "training_examples = generate_training_examples(raw_dataset, num_examples=2000)\n",
    "print(f\"‚úÖ Generated {len(training_examples)} training examples\")\n",
    "\n",
    "# Show task distribution\n",
    "task_counts = {}\n",
    "for ex in training_examples:\n",
    "    task = ex['task']\n",
    "    task_counts[task] = task_counts.get(task, 0) + 1\n",
    "\n",
    "print(\"\\nTask distribution:\")\n",
    "for task, count in task_counts.items():\n",
    "    print(f\"  {task}: {count} ({100*count/len(training_examples):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model with Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "compute_dtype = getattr(torch, config.bnb_4bit_compute_dtype)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=config.use_4bit,\n",
    "    bnb_4bit_quant_type=config.bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=compute_dtype,\n",
    "    bnb_4bit_use_double_quant=config.use_nested_quant,\n",
    ")\n",
    "\n",
    "print(\"Quantization config:\")\n",
    "print(f\"  4-bit: {config.use_4bit}\")\n",
    "print(f\"  Quant type: {config.bnb_4bit_quant_type}\")\n",
    "print(f\"  Compute dtype: {config.bnb_4bit_compute_dtype}\")\n",
    "print(f\"  Nested quantization: {config.use_nested_quant}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "print(f\"\\nLoading tokenizer: {config.model_name}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    config.model_name,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Set padding token if not present\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "print(f\"  Vocab size: {len(tokenizer)}\")\n",
    "print(f\"  Pad token: {tokenizer.pad_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with quantization\n",
    "print(f\"\\nLoading model: {config.model_name}\")\n",
    "print(\"‚ö†Ô∏è  This may take several minutes...\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    config.model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=compute_dtype,\n",
    ")\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "print(f\"‚úÖ Model loaded and quantized\")\n",
    "print(f\"  Model memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA\n",
    "peft_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(\"‚úÖ LoRA adapters added\")\n",
    "print(f\"\\nTrainable parameters:\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(example: Dict[str, str]) -> str:\n",
    "    \"\"\"Format example as instruction for Qwen.\"\"\"\n",
    "    return f\"\"\"<|im_start|>system\n",
    "You are Qwen, an AI assistant specialized in log analysis and understanding.<|im_end|>\n",
    "<|im_start|>user\n",
    "{example['instruction']}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "{example['response']}<|im_end|>\"\"\"\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "train_dataset = Dataset.from_list(training_examples)\n",
    "\n",
    "# Split into train/eval\n",
    "split_dataset = train_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_data = split_dataset['train']\n",
    "eval_data = split_dataset['test']\n",
    "\n",
    "print(f\"‚úÖ Dataset prepared\")\n",
    "print(f\"  Training examples: {len(train_data)}\")\n",
    "print(f\"  Evaluation examples: {len(eval_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize datasets\n",
    "print(\"Tokenizing datasets...\")\n",
    "\n",
    "def tokenize_batch(batch):\n",
    "    texts = [format_instruction({\n",
    "        'instruction': batch['instruction'][i],\n",
    "        'response': batch['response'][i]\n",
    "    }) for i in range(len(batch['instruction']))]\n",
    "    \n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=config.max_seq_length,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "tokenized_train = train_data.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=train_data.column_names,\n",
    "    desc=\"Tokenizing training data\",\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_data.map(\n",
    "    tokenize_batch,\n",
    "    batched=True,\n",
    "    remove_columns=eval_data.column_names,\n",
    "    desc=\"Tokenizing evaluation data\",\n",
    ")\n",
    "\n",
    "# Add labels\n",
    "tokenized_train = tokenized_train.map(\n",
    "    lambda x: {\"labels\": x[\"input_ids\"]},\n",
    "    desc=\"Adding labels\",\n",
    ")\n",
    "tokenized_eval = tokenized_eval.map(\n",
    "    lambda x: {\"labels\": x[\"input_ids\"]},\n",
    "    desc=\"Adding labels\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Tokenization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=config.output_dir,\n",
    "    num_train_epochs=config.num_train_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    learning_rate=config.learning_rate,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    weight_decay=config.weight_decay,\n",
    "    logging_steps=config.logging_steps,\n",
    "    save_steps=config.save_steps,\n",
    "    eval_steps=config.eval_steps,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    save_total_limit=3,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training arguments configured\")\n",
    "effective_batch_size = config.per_device_train_batch_size * config.gradient_accumulation_steps\n",
    "print(f\"  Effective batch size: {effective_batch_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Causal LM (not masked LM)\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training on {len(tokenized_train)} examples\")\n",
    "print(f\"Evaluating on {len(tokenized_eval)} examples\")\n",
    "print(f\"Number of epochs: {config.num_train_epochs}\")\n",
    "print(\"\\n‚ö†Ô∏è  Training may take several hours depending on your GPU...\\n\")\n",
    "\n",
    "# Train\n",
    "train_result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"Training runtime: {train_result.metrics['train_runtime']:.2f}s\")\n",
    "print(f\"Samples per second: {train_result.metrics['train_samples_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "print(\"\\nEvaluating model...\")\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "output_dir = config.output_dir\n",
    "print(f\"\\nSaving model to {output_dir}...\")\n",
    "\n",
    "# Save the final model\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"‚úÖ Model saved to {output_dir}\")\n",
    "\n",
    "# Save training info\n",
    "training_info = {\n",
    "    'model_name': config.model_name,\n",
    "    'dataset_name': config.dataset_name,\n",
    "    'num_train_examples': len(tokenized_train),\n",
    "    'num_eval_examples': len(tokenized_eval),\n",
    "    'training_loss': train_result.training_loss,\n",
    "    'eval_loss': eval_results['eval_loss'],\n",
    "    'lora_r': config.lora_r,\n",
    "    'lora_alpha': config.lora_alpha,\n",
    "}\n",
    "\n",
    "with open(f\"{output_dir}/training_info.json\", 'w') as f:\n",
    "    json.dump(training_info, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Training info saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test inference function\n",
    "def test_model(task_type: str = \"parsing\"):\n",
    "    \"\"\"Test the fine-tuned model on different tasks.\"\"\"\n",
    "    \n",
    "    # Sample test cases\n",
    "    test_cases = {\n",
    "        'parsing': [\n",
    "            \"2024-01-15 14:23:45 ERROR Connection to 192.168.1.100 failed\",\n",
    "            \"2024-01-15 14:23:46 INFO User login successful from 10.0.0.5\",\n",
    "        ],\n",
    "        'classification': [\n",
    "            \"2024-01-15 14:23:45 ERROR Database connection timeout\",\n",
    "            \"2024-01-15 14:23:46 INFO Service started successfully\",\n",
    "        ],\n",
    "        'summarization': [\n",
    "            \"\"\"1. 2024-01-15 10:00:00 INFO Service starting\\n2. 2024-01-15 10:00:01 INFO Loading configuration\\n3. 2024-01-15 10:00:02 ERROR Database connection failed\\n4. 2024-01-15 10:00:03 WARN Retrying connection\\n5. 2024-01-15 10:00:04 INFO Connection established\"\"\"\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    if task_type == 'parsing':\n",
    "        instruction_template = (\n",
    "            \"You are a log parsing expert. Extract the template from the following log line by \"\n",
    "            \"replacing variable parts with placeholders.\\n\\nLog line: {}\\n\\nProvide only the extracted template.\"\n",
    "        )\n",
    "    elif task_type == 'classification':\n",
    "        instruction_template = (\n",
    "            \"You are a log analysis expert. Classify the following log entry as 'normal' or 'anomaly'.\\n\\n\"\n",
    "            \"Log entry: {}\\n\\nClassification:\"\n",
    "        )\n",
    "    else:  # summarization\n",
    "        instruction_template = (\n",
    "            \"You are a log analysis expert. Summarize the following sequence of log entries.\\n\\n\"\n",
    "            \"Logs:\\n{}\\n\\nSummary:\"\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Testing {task_type.upper()} Task\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    for i, test_input in enumerate(test_cases[task_type], 1):\n",
    "        instruction = instruction_template.format(test_input)\n",
    "        \n",
    "        prompt = f\"\"\"<|im_start|>system\n",
    "You are Qwen, an AI assistant specialized in log analysis and understanding.<|im_end|>\n",
    "<|im_start|>user\n",
    "{instruction}<|im_end|>\n",
    "<|im_start|>assistant\\n\"\"\"\n",
    "        \n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=tokenizer.pad_token_id,\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "        # Extract only the assistant's response\n",
    "        if \"<|im_start|>assistant\" in response:\n",
    "            response = response.split(\"<|im_start|>assistant\\n\")[-1]\n",
    "        if \"<|im_end|>\" in response:\n",
    "            response = response.split(\"<|im_end|>\")[0]\n",
    "        \n",
    "        print(f\"Test Case {i}:\")\n",
    "        print(f\"Input: {test_input[:200]}...\" if len(test_input) > 200 else f\"Input: {test_input}\")\n",
    "        print(f\"Output: {response.strip()}\")\n",
    "        print()\n",
    "\n",
    "# Test all task types\n",
    "for task in ['parsing', 'classification', 'summarization']:\n",
    "    test_model(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n‚úÖ Successfully fine-tuned {config.model_name}\")\n",
    "print(f\"\\nüìä Training Statistics:\")\n",
    "print(f\"  - Training examples: {len(tokenized_train)}\")\n",
    "print(f\"  - Evaluation examples: {len(tokenized_eval)}\")\n",
    "print(f\"  - Training loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  - Evaluation loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"  - Training time: {train_result.metrics['train_runtime']:.2f}s\")\n",
    "print(f\"\\nüíæ Model saved to: {config.output_dir}\")\n",
    "\n",
    "print(f\"\\nüéØ Capabilities:\")\n",
    "print(\"  ‚úÖ Log Parsing - Extract templates from raw logs\")\n",
    "print(\"  ‚úÖ Log Classification - Detect anomalies\")\n",
    "print(\"  ‚úÖ Log Summarization - Summarize log sequences\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(\"  1. Test the model on your own log data\")\n",
    "print(\"  2. Fine-tune further with more domain-specific data\")\n",
    "print(\"  3. Deploy the model for production use\")\n",
    "print(\"  4. Integrate with your log analysis pipeline\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(f\"\\nüí° Tip: Download the model from {config.output_dir} or save to Google Drive\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Notes\n",
    "\n",
    "### SoftMoE Integration\n",
    "The SoftMoE layer implementation is included but not fully integrated into the model architecture. For production use, you would need to:\n",
    "1. Hook the SoftMoE layers into specific transformer layers\n",
    "2. Add forward hooks to apply MoE routing\n",
    "3. Implement load balancing loss for expert utilization\n",
    "\n",
    "### Memory Optimization\n",
    "- Reduce `max_seq_length` if running out of memory\n",
    "- Decrease `per_device_train_batch_size` or increase `gradient_accumulation_steps`\n",
    "- Use gradient checkpointing for larger models\n",
    "\n",
    "### Performance Tips\n",
    "- Use A100 or V100 GPU for faster training\n",
    "- Enable flash attention for better performance\n",
    "- Use mixed precision training (already enabled with fp16)\n",
    "\n",
    "### Dataset Customization\n",
    "Replace `config.dataset_name` with your own HuggingFace dataset containing log data with these columns:\n",
    "- `content`: The raw log text\n",
    "- `host`: (optional) Host/server name\n",
    "- `log_type`: (optional) Type of log\n",
    "\n",
    "### Citation\n",
    "If you use this notebook, please cite:\n",
    "```\n",
    "Qwen2.5: https://github.com/QwenLM/Qwen\n",
    "QLoRA: https://arxiv.org/abs/2305.14314\n",
    "SoftMoE: https://arxiv.org/abs/2308.00951\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}