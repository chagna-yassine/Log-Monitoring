"""
Simple BGL Benchmarking

Runs benchmarking on the BGL test data generated by test_bgl_direct.py
"""

import pandas as pd
import yaml
from pathlib import Path
import sys
import json
import time

# Add the src directory to the Python path
sys.path.append(str(Path(__file__).resolve().parents[1] / 'src'))

try:
    from model.inference import LogAnomalyDetector
    from evaluation.metrics import BenchmarkMetrics
except ImportError as e:
    print(f"Import error: {e}")
    print("Make sure you're running from the project root directory")
    sys.exit(1)


def load_config(config_path: str = "config.yaml") -> dict:
    """Load configuration from YAML file."""
    with open(config_path, 'r') as f:
        return yaml.safe_load(f)


def main():
    """Run BGL benchmarking."""
    print("="*70)
    print("SIMPLE BGL BENCHMARKING")
    print("="*70)

    # Setup paths
    bgl_output_dir = Path("datasets/bgl/output/bgl")
    test_file = bgl_output_dir / "bgl_test.csv"
    results_dir = Path("results")
    results_dir.mkdir(parents=True, exist_ok=True)

    # Check if test file exists
    if not test_file.exists():
        print(f"Error: BGL test file not found: {test_file}")
        print("Please run: python scripts/test_bgl_direct.py")
        sys.exit(1)

    # Load configuration
    config = load_config()
    model_config = config['model']

    print(f"Loading BGL test data from: {test_file}")
    test_df = pd.read_csv(test_file)
    
    if test_df.empty:
        print("Error: BGL test DataFrame is empty.")
        sys.exit(1)

    # Prepare data
    if 'Text' in test_df.columns:
        texts = test_df['Text'].tolist()
    elif 'TextSequence' in test_df.columns:
        texts = test_df['TextSequence'].tolist()
    else:
        print("Error: No text column found in test data")
        print(f"Available columns: {test_df.columns.tolist()}")
        sys.exit(1)
    
    labels = test_df['Label'].tolist()

    print(f"Loaded {len(texts):,} BGL test samples")
    print(f"Anomaly rate: {test_df['Label'].mean():.2%} (1=Anomaly)")
    print(f"Normal samples: {(test_df['Label'] == 0).sum():,}")
    print(f"Anomaly samples: {(test_df['Label'] == 1).sum():,}")

    # Initialize model
    print(f"\n" + "="*70)
    print(f"STEP 1: MODEL INFERENCE ({model_config['name']})")
    print(f"="*70)
    
    try:
        detector = LogAnomalyDetector(
            model_name=model_config['name'],
            max_length=model_config['max_length'],
            batch_size=model_config['batch_size'],
            device=model_config['device']
        )
        print("✓ Model loaded successfully")
    except Exception as e:
        print(f"Error loading model: {e}")
        print("Make sure the model is available and dependencies are installed")
        sys.exit(1)

    # Run inference
    print("Running inference...")
    start_time = time.time()
    
    try:
        predictions, probabilities = detector.predict(texts)
        end_time = time.time()
        inference_time = end_time - start_time
        
        print(f"✓ Inference complete in {inference_time:.2f} seconds")
        print(f"✓ Throughput: {len(texts) / inference_time:.2f} samples/second")
        
    except Exception as e:
        print(f"Error during inference: {e}")
        sys.exit(1)

    # Compute metrics
    print(f"\n" + "="*70)
    print("STEP 2: EVALUATION METRICS")
    print("="*70)
    
    try:
        metrics_calculator = BenchmarkMetrics()
        
        # Compute all metrics
        results = metrics_calculator.compute_all_metrics(
            y_true=labels,
            y_pred=predictions,
            y_proba=probabilities[:, 1],  # Probability of positive class (Anomaly)
            inference_time=inference_time,
            num_samples=len(texts)
        )
        
        print("✓ Metrics computed successfully")
        
    except Exception as e:
        print(f"Error computing metrics: {e}")
        sys.exit(1)

    # Add metadata
    results['model'] = model_config
    results['dataset'] = {
        'name': 'BGL',
        'test_file': str(test_file),
        'num_samples': len(texts)
    }
    results['timestamp'] = pd.Timestamp.now().isoformat()
    results['configuration'] = {
        'batch_size': model_config['batch_size'],
        'max_length': model_config['max_length']
    }

    # Save results
    results_file = results_dir / "bgl_benchmark_results.json"
    with open(results_file, 'w') as f:
        json.dump(results, f, indent=2)
    print(f"✓ Benchmark results saved to: {results_file}")

    # Save predictions
    predictions_df = pd.DataFrame({
        'Text': texts,
        'TrueLabel': labels,
        'PredictedLabel': predictions,
        'AnomalyProbability': probabilities[:, 1]
    })
    predictions_file = results_dir / "bgl_predictions.csv"
    predictions_df.to_csv(predictions_file, index=False)
    print(f"✓ Predictions saved to: {predictions_file}")

    # Display key results
    print(f"\n" + "="*70)
    print("BGL BENCHMARK RESULTS")
    print("="*70)
    
    metrics = results['metrics']
    print(f"\nKey Performance Metrics:")
    print(f"  Accuracy:        {metrics['accuracy']:.4f}")
    print(f"  Precision:       {metrics['precision_binary']:.4f}")
    print(f"  Recall:          {metrics['recall_binary']:.4f}")
    print(f"  F1-Score:        {metrics['f1_binary']:.4f}")
    print(f"  AUC-ROC:         {metrics['auc_roc']:.4f}")
    
    print(f"\nPerformance Metrics:")
    print(f"  Inference Time:  {metrics['inference_time_seconds']:.2f} seconds")
    print(f"  Throughput:      {metrics['throughput_samples_per_second']:.2f} samples/second")
    
    if 'per_class' in metrics:
        print(f"\nPer-Class Performance:")
        for class_name, class_metrics in metrics['per_class'].items():
            print(f"  {class_name}:")
            print(f"    Precision: {class_metrics['precision']:.4f}")
            print(f"    Recall:    {class_metrics['recall']:.4f}")
            print(f"    F1-Score:  {class_metrics['f1_score']:.4f}")
            print(f"    Support:   {class_metrics['support']:,}")

    print("\n" + "="*70)
    print("BGL BENCHMARKING COMPLETE!")
    print("="*70)
    
    print(f"\nFiles generated:")
    print(f"  Results: {results_file}")
    print(f"  Predictions: {predictions_file}")


if __name__ == "__main__":
    main()
