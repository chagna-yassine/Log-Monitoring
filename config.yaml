# Log Anomaly Detection Benchmarking Configuration

# Model Configuration
model:
  name: "Dumi2025/log-anomaly-detection-model-roberta"
  max_length: 512
  batch_size: 32
  device: "cuda"  # or "cpu"

# Dataset Selection (HDFS, BGL, or AIT)
dataset_name: "AIT"  # Change to "HDFS", "BGL", or "AIT"

# HDFS Dataset Configuration
hdfs_dataset:
  name: "HDFS"
  url: "https://zenodo.org/record/3227177/files/HDFS_1.tar.gz"
  base_path: "datasets/hdfs"
  raw_log: "HDFS.log"
  label_file: "anomaly_label.csv"

# BGL Dataset Configuration
bgl_dataset:
  name: "BGL"
  url: "https://zenodo.org/records/8196385/files/BGL.zip?download=1"
  base_path: "datasets/bgl"
  raw_log: "BGL_2k.log"
  structured_file: "BGL_2k.log_structured.csv"  # BGL structured file with labels
  templates_file: "BGL_2k.log_templates.csv"    # BGL templates file

# AIT-LDS Dataset Configuration
ait_dataset:
  name: "AIT-LDS"
  base_url: "https://zenodo.org/record/5789063/files"
  base_path: "datasets/ait"
  datasets:
    - name: "fox"
      url: "https://zenodo.org/records/5789063/files/fox.tar.gz?download=1"
      size: "26 GB"
      scan_volume: "High"
    - name: "harrison"
      url: "https://zenodo.org/records/5789063/files/harrison.tar.gz?download=1"
      size: "27 GB"
      scan_volume: "High"
    - name: "russellmitchell"
      url: "https://zenodo.org/records/5789063/files/russellmitchell.tar.gz?download=1"
      size: "14 GB"
      scan_volume: "Low"
    - name: "santos"
      url: "https://zenodo.org/records/5789063/files/santos.tar.gz?download=1"
      size: "17 GB"
      scan_volume: "Low"
    - name: "shaw"
      url: "https://zenodo.org/records/5789063/files/shaw.tar.gz?download=1"
      size: "27 GB"
      scan_volume: "Low"
    - name: "wardbeck"
      url: "https://zenodo.org/records/5789063/files/wardbeck.tar.gz?download=1"
      size: "26 GB"
      scan_volume: "Low"
    - name: "wheeler"
      url: "https://zenodo.org/records/5789063/files/wheeler.tar.gz?download=1"
      size: "30 GB"
      scan_volume: "High"
    - name: "wilson"
      url: "https://zenodo.org/records/5789063/files/wilson.tar.gz?download=1"
      size: "39 GB"
      scan_volume: "High"
  selected_dataset: "russellmitchell"  # Start with smallest dataset for testing
  log_types: ["apache_access", "apache_error", "auth", "dns", "vpn", "audit", "suricata", "syslog"]
  attack_types: ["scan", "webshell_upload", "password_cracking", "privilege_escalation", "remote_execution", "data_exfiltration"]
  
# Output Configuration
output:
  hdfs:
    base_path: "datasets/hdfs/output/hdfs"
    structured_log: "HDFS.log_structured.csv"
    templates: "HDFS.log_templates.csv"
    sequences: "hdfs_sequence.csv"
    template_mapping: "hdfs_log_templates.json"
  bgl:
    base_path: "datasets/bgl/output/bgl"
    structured_log: "BGL.log_structured.csv"
    templates: "BGL.log_templates.csv"
    sequences: "bgl_sequence.csv"
    template_mapping: "bgl_log_templates.json"
  ait:
    base_path: "datasets/ait/output/ait"
    structured_log: "ait_structured.csv"
    templates: "ait_templates.csv"
    sequences: "ait_sequence.csv"
    template_mapping: "ait_log_templates.json"
  results_dir: "results"

# Preprocessing Configuration
preprocessing:
  # Drain parser parameters
  drain:
    depth: 4
    st: 0.4  # Similarity threshold
    rex:
      - "(?<=blk_)[-\\d]+"
      - "\\d+\\.\\d+\\.\\d+\\.\\d+"
      - "(/[-\\w]+)+"
  
  # Sequence parameters
  sequence_separator: " | "
  
  # Train/Test split
  train_ratio: 0.7  # 70% of normal blocks for training
  random_seed: 42

# Benchmark Configuration
benchmark:
  metrics:
    - "accuracy"
    - "precision"
    - "recall"
    - "f1"
    - "auc_roc"
    - "confusion_matrix"
  averaging_methods:
    - "binary"
    - "weighted"
    - "macro"
    - "micro"

