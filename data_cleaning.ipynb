{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# AIT Dataset - Data Loading and Cleaning\n",
        "\n",
        "This notebook loads the AIT Fox dataset from HuggingFace and prepares it for analysis and cleaning.\n",
        "\n",
        "**Dataset:** chYassine/ait-fox-raw-v02\n",
        "- **Total Logs:** ~5.4M entries\n",
        "- **Hosts:** 21 unique hosts\n",
        "- **Log Types:** 11 different types\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_colwidth', 100)\n",
        "pd.set_option('display.max_rows', 50)\n",
        "\n",
        "print(\"✅ Libraries imported successfully\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Dataset from HuggingFace\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset repository name\n",
        "dataset_repo = \"chYassine/ait-fox-raw-v02\"\n",
        "\n",
        "print(f\"Loading dataset: {dataset_repo}\")\n",
        "print(\"This may take a few minutes...\\n\")\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(dataset_repo, split='train')\n",
        "\n",
        "print(f\"✅ Dataset loaded successfully!\")\n",
        "print(f\"   Total entries: {len(dataset):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Convert to Pandas DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to DataFrame\n",
        "print(\"Converting to DataFrame...\")\n",
        "df = pd.DataFrame(dataset)\n",
        "\n",
        "print(f\"✅ DataFrame created successfully!\")\n",
        "print(f\"   Shape: {df.shape}\")\n",
        "print(f\"   Columns: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Initial Data Exploration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display first few rows\n",
        "print(\"First 5 rows of the dataset:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Basic information\n",
        "print(\"Dataset Information:\")\n",
        "df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data types of each column\n",
        "print(\"\\nData Types:\")\n",
        "print(df.dtypes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Statistical summary for numerical columns\n",
        "print(\"Statistical Summary:\")\n",
        "df.describe()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Check for Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "missing_values = df.isnull().sum()\n",
        "missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
        "\n",
        "missing_df = pd.DataFrame({\n",
        "    'Column': missing_values.index,\n",
        "    'Missing Count': missing_values.values,\n",
        "    'Missing Percentage': missing_percentage.values\n",
        "})\n",
        "\n",
        "print(\"Missing Values Analysis:\")\n",
        "print(missing_df[missing_df['Missing Count'] > 0])\n",
        "\n",
        "if missing_df['Missing Count'].sum() == 0:\n",
        "    print(\"\\n✅ No missing values found!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Check for Duplicates\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for duplicate rows\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Total duplicate rows: {duplicates:,}\")\n",
        "print(f\"Percentage of duplicates: {(duplicates / len(df)) * 100:.2f}%\")\n",
        "\n",
        "# Check for duplicates based on specific columns (e.g., content + host + log_type)\n",
        "if 'content' in df.columns and 'host' in df.columns and 'log_type' in df.columns:\n",
        "    duplicates_subset = df.duplicated(subset=['content', 'host', 'log_type']).sum()\n",
        "    print(f\"\\nDuplicates based on content+host+log_type: {duplicates_subset:,}\")\n",
        "    print(f\"Percentage: {(duplicates_subset / len(df)) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Categorical Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze host distribution\n",
        "if 'host' in df.columns:\n",
        "    print(\"Host Distribution:\")\n",
        "    host_counts = df['host'].value_counts()\n",
        "    print(f\"\\nTotal unique hosts: {len(host_counts)}\")\n",
        "    print(f\"\\nTop 10 Hosts:\")\n",
        "    print(host_counts.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize host distribution\n",
        "if 'host' in df.columns:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    host_counts.head(15).plot(kind='bar')\n",
        "    plt.title('Top 15 Hosts by Log Count')\n",
        "    plt.xlabel('Host')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze log type distribution\n",
        "if 'log_type' in df.columns:\n",
        "    print(\"Log Type Distribution:\")\n",
        "    log_type_counts = df['log_type'].value_counts()\n",
        "    print(f\"\\nTotal unique log types: {len(log_type_counts)}\")\n",
        "    print(f\"\\nLog Type Breakdown:\")\n",
        "    print(log_type_counts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize log type distribution\n",
        "if 'log_type' in df.columns:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    log_type_counts.plot(kind='bar')\n",
        "    plt.title('Log Type Distribution')\n",
        "    plt.xlabel('Log Type')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze binary vs text logs\n",
        "if 'is_binary' in df.columns:\n",
        "    print(\"Binary vs Text Logs:\")\n",
        "    binary_counts = df['is_binary'].value_counts()\n",
        "    print(binary_counts)\n",
        "    print(f\"\\nText logs: {binary_counts.get(False, 0):,}\")\n",
        "    print(f\"Binary files: {binary_counts.get(True, 0):,}\")\n",
        "    print(f\"Binary percentage: {(binary_counts.get(True, 0) / len(df)) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Content Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze content length for text logs\n",
        "if 'content' in df.columns and 'is_binary' in df.columns:\n",
        "    # Filter text logs only\n",
        "    text_logs = df[df['is_binary'] == False].copy()\n",
        "    \n",
        "    # Calculate content length\n",
        "    text_logs['content_length'] = text_logs['content'].astype(str).str.len()\n",
        "    \n",
        "    print(\"Content Length Analysis (Text Logs Only):\")\n",
        "    print(f\"Total text logs: {len(text_logs):,}\")\n",
        "    print(f\"\\nContent Length Statistics:\")\n",
        "    print(text_logs['content_length'].describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize content length distribution\n",
        "if 'content' in df.columns and 'is_binary' in df.columns:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.hist(text_logs['content_length'], bins=50, edgecolor='black')\n",
        "    plt.title('Distribution of Content Length (Text Logs)')\n",
        "    plt.xlabel('Content Length (characters)')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.xlim(0, text_logs['content_length'].quantile(0.95))  # Limit to 95th percentile for better visualization\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check for empty or null content\n",
        "if 'content' in df.columns:\n",
        "    empty_content = df[df['content'].isna() | (df['content'].astype(str).str.strip() == '')]\n",
        "    print(f\"Rows with empty or null content: {len(empty_content):,}\")\n",
        "    print(f\"Percentage: {(len(empty_content) / len(df)) * 100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Cross-Analysis: Host vs Log Type\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-tabulation of host and log_type\n",
        "if 'host' in df.columns and 'log_type' in df.columns:\n",
        "    print(\"Host-Log Type Cross Analysis:\")\n",
        "    cross_tab = pd.crosstab(df['host'], df['log_type'])\n",
        "    print(\"\\nCross-tabulation (showing first 10 hosts):\")\n",
        "    print(cross_tab.head(10))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hosts with most log type variety\n",
        "if 'host' in df.columns and 'log_type' in df.columns:\n",
        "    host_log_variety = df.groupby('host')['log_type'].nunique().sort_values(ascending=False)\n",
        "    print(\"Hosts with Most Log Type Variety:\")\n",
        "    print(host_log_variety)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Sample Data Inspection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display random samples from different log types\n",
        "if 'log_type' in df.columns and 'content' in df.columns:\n",
        "    print(\"Sample Logs from Each Log Type:\\n\")\n",
        "    log_types = df['log_type'].unique()\n",
        "    \n",
        "    for log_type in sorted(log_types)[:5]:  # Show first 5 log types\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Log Type: {log_type}\")\n",
        "        print('='*80)\n",
        "        sample = df[df['log_type'] == log_type].sample(n=1).iloc[0]\n",
        "        print(f\"Host: {sample.get('host', 'N/A')}\")\n",
        "        print(f\"Path: {sample.get('path', 'N/A')}\")\n",
        "        content = str(sample.get('content', 'N/A'))\n",
        "        print(f\"Content: {content[:300]}...\") if len(content) > 300 else print(f\"Content: {content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Data Cleaning Operations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.1 Remove Duplicates (if needed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Remove duplicate rows\n",
        "# Uncomment the following lines if you want to remove duplicates\n",
        "\n",
        "# print(f\"Original shape: {df.shape}\")\n",
        "# df_cleaned = df.drop_duplicates()\n",
        "# print(f\"After removing duplicates: {df_cleaned.shape}\")\n",
        "# print(f\"Removed {len(df) - len(df_cleaned):,} duplicate rows\")\n",
        "\n",
        "# Or remove duplicates based on specific columns:\n",
        "# df_cleaned = df.drop_duplicates(subset=['content', 'host', 'log_type'], keep='first')\n",
        "\n",
        "print(\"Duplicate removal cell (currently commented out)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.2 Handle Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle missing values\n",
        "# Uncomment and modify based on your needs\n",
        "\n",
        "# Option 1: Drop rows with missing values in specific columns\n",
        "# df_cleaned = df.dropna(subset=['content', 'host'])\n",
        "\n",
        "# Option 2: Fill missing values\n",
        "# df_cleaned = df.fillna({\n",
        "#     'content': '',\n",
        "#     'host': 'unknown',\n",
        "#     'log_type': 'unknown'\n",
        "# })\n",
        "\n",
        "# Option 3: Drop all rows with any missing values\n",
        "# df_cleaned = df.dropna()\n",
        "\n",
        "print(\"Missing values handling cell (currently commented out)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.3 Filter Binary Files (if needed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter out binary files to keep only text logs\n",
        "if 'is_binary' in df.columns:\n",
        "    # Uncomment to filter out binary files\n",
        "    # df_text_only = df[df['is_binary'] == False].copy()\n",
        "    # print(f\"Original dataset: {len(df):,} rows\")\n",
        "    # print(f\"Text-only dataset: {len(df_text_only):,} rows\")\n",
        "    # print(f\"Removed {len(df) - len(df_text_only):,} binary files\")\n",
        "    \n",
        "    print(\"Binary filter cell (currently commented out)\")\n",
        "else:\n",
        "    print(\"'is_binary' column not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.4 Filter by Host or Log Type\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter data by specific hosts or log types\n",
        "\n",
        "# Example: Keep only specific hosts\n",
        "# hosts_to_keep = ['monitoring', 'inet-firewall', 'vpn', 'webserver']\n",
        "# df_filtered = df[df['host'].isin(hosts_to_keep)]\n",
        "\n",
        "# Example: Keep only specific log types\n",
        "# log_types_to_keep = ['suricata', 'apache2', 'audit']\n",
        "# df_filtered = df[df['log_type'].isin(log_types_to_keep)]\n",
        "\n",
        "# Example: Exclude specific hosts\n",
        "# hosts_to_exclude = ['attacker_0']\n",
        "# df_filtered = df[~df['host'].isin(hosts_to_exclude)]\n",
        "\n",
        "print(\"Filtering cell (currently commented out)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.5 Clean Content Field\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean the content field\n",
        "if 'content' in df.columns:\n",
        "    # Example cleaning operations (uncomment as needed)\n",
        "    \n",
        "    # Remove leading/trailing whitespace\n",
        "    # df['content'] = df['content'].astype(str).str.strip()\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    # df['content'] = df['content'].astype(str).str.replace(r'\\s+', ' ', regex=True)\n",
        "    \n",
        "    # Remove specific characters or patterns\n",
        "    # df['content'] = df['content'].astype(str).str.replace(r'[^\\x00-\\x7F]+', '', regex=True)  # Remove non-ASCII\n",
        "    \n",
        "    # Convert to lowercase (if case-insensitive analysis is needed)\n",
        "    # df['content_lower'] = df['content'].astype(str).str.lower()\n",
        "    \n",
        "    print(\"Content cleaning cell (currently commented out)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 11.6 Create Derived Features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create useful derived features\n",
        "if 'content' in df.columns:\n",
        "    # Add content length\n",
        "    # df['content_length'] = df['content'].astype(str).str.len()\n",
        "    \n",
        "    # Add word count\n",
        "    # df['word_count'] = df['content'].astype(str).str.split().str.len()\n",
        "    \n",
        "    # Extract timestamp if present in content (example pattern)\n",
        "    # df['timestamp'] = df['content'].astype(str).str.extract(r'(\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2})')\n",
        "    \n",
        "    print(\"Feature engineering cell (currently commented out)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Export Cleaned Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export cleaned data to CSV\n",
        "# Uncomment to save the cleaned data\n",
        "\n",
        "# output_file = 'cleaned_ait_fox_data.csv'\n",
        "# df.to_csv(output_file, index=False)\n",
        "# print(f\"✅ Cleaned data saved to: {output_file}\")\n",
        "\n",
        "# For large datasets, consider using parquet format\n",
        "# output_file = 'cleaned_ait_fox_data.parquet'\n",
        "# df.to_parquet(output_file, index=False)\n",
        "# print(f\"✅ Cleaned data saved to: {output_file}\")\n",
        "\n",
        "print(\"Export cell (currently commented out)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Summary Statistics After Cleaning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display final summary after cleaning\n",
        "print(\"Final Dataset Summary:\")\n",
        "print(f\"\\nShape: {df.shape}\")\n",
        "print(f\"Total rows: {len(df):,}\")\n",
        "print(f\"Total columns: {len(df.columns)}\")\n",
        "print(f\"\\nColumns: {list(df.columns)}\")\n",
        "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / (1024**2):.2f} MB\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
